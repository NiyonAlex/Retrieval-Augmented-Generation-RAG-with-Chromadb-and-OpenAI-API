# -*- coding: utf-8 -*-
"""Retrieval-Augmented Generation (RAG) with Chromadb and OpenAI API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EsMMpFx_KR5edkWMCm7QGi9SpOTPT7YP

This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using:

- Chromadb for efficient vector-based retrieval.
- OpenAI API for response generation.
- T5 (FLAN-T5-small) for query expansion.
- Sentence-BERT (SBERT) for embedding generation.

# 1. Install and Import Dependencies

First, ensure that all required libraries are installed before running the code.
"""

"""Now, import the necessary libraries:"""

import os
import openai
import chromadb
import json
import numpy as np
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from google.colab import files

"""# 2. Set Up OpenAI API Key

Replace 'your-api-key' with your actual OpenAI API key.
"""

openai.api_key = "Your-api-key"

"""# 3. Initialize ChromaDB

We use ChromaDB as a vector database and SBERT (all-MiniLM-L6-v2) for text embeddings.

"""

# Initialize ChromaDB client and collection
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(
    name="rag_collection",
    embedding_function=SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
)

"""# 4. Query Expansion using FLAN-T5
FLAN-T5 helps rewrite the user query for better document retrieval.
"""

# Load query expansion model (FLAN-T5)
query_expansion_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

# Expand the query to improve search results
def expand_query(query):
    input_text = f"Expand this query: {query}"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids
    output_ids = query_expansion_model.generate(input_ids, max_length=50)
    expanded_query = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return expanded_query

"""# 5.  Store Documents in ChromaDB
Here, we add sample knowledge base documents to ChromaDB.
"""

documents = [
    "AI is transforming the world with automation and intelligence.",
    "Machine learning helps in predictive analytics and decision-making.",
    "Retrieval-Augmented Generation (RAG) improves LLM response accuracy.",
    "Vector databases store embeddings for efficient information retrieval."
]

# Store documents in ChromaDB
for i, doc in enumerate(documents):
    collection.add(ids=[str(i)], documents=[doc])

"""# 6. Retrieve Similar Documents
Now, we retrieve the top K most relevant documents using semantic search.


"""

def retrieve_documents(query, k=2):
    results = collection.query(
        query_texts=[query],
        n_results=k
    )
    return results["documents"][0] if results["documents"] else []

"""# 7. Generate Responses using OpenAI API
The retrieved documents act as context for the AI model.
"""

def generate_response(query, retrieved_docs):
    context = "\n".join(retrieved_docs)
    prompt = f"Context: {context}\n\nUser Query: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": prompt}]
    )
    return response['choices'][0]['message']['content']

"""# 8. Run a Sample Query


*   Expands the query
*   Retrieves relevant documents
*   Generates a response




"""

query = "How does RAG improve AI?"
expanded_query = expand_query(query)
retrieved_docs = retrieve_documents(expanded_query)
response = generate_response(query, retrieved_docs)

# Prepare result data
result_data = {
    "Original Query": query,
    "Expanded Query": expanded_query,
    "Retrieved Documents": retrieved_docs,
    "Generated Response": response
}

# Print the results
print("Original Query:", query)
print("Expanded Query:", expanded_query)
print("Retrieved Docs:", retrieved_docs)
print("Generated Response:", response)

"""# 9. Code to Save and Download Results
Add this after running the query to save the results and download the file:
"""

# Save as a text file
with open("RAG_result.txt", "w") as f:
    for key, value in result_data.items():
        f.write(f"{key}: {value}\n\n")

# Save as a JSON file
with open("RAG_result.json", "w") as f:
    json.dump(result_data, f, indent=4)

# Download the files
files.download("RAG_result.txt")
files.download("RAG_result.json")

"""ðŸ”¹ What This Code Does
1. Uses ChromaDB instead of FAISS for storing and retrieving vector embeddings.
2. Expands user queries using FLAN-T5.
3. Stores and retrieves documents using SBERT embeddings.
4. Uses OpenAI API to generate responses.
5. Saves the vector database persistently.

Why Use ChromaDB?
1. Easier to manage than FAISS (No need to manually store index files).
2. Scalable (Works in cloud environments).
3. Flexible (Supports multiple embedding models).
4. Persistent storage (Unlike FAISS, it keeps data even after restarting).
"""